combine_strategy: "mean_prob"
pad_to_multiple_of: 128
output: outputs/eval_lockstep

eval:
  tasks: [arc_easy,arc_challenge,piqa,boolq,arithmetic,mmlu,ifeval,agieval_en,agieval_cn]
  lengths: [128, 256, 512, 1024, 2048]
  tokens_per_batch: 4096
  add_bos: true
  chat_template_mode: surround_instruct
  confirm_run_unsafe_code: true

models:
  - pretrained_model_name_or_path: "benjamin/Llama-3.2-3B-Instruct-flax" # pivot first
    tokenizer_name: "meta-llama/Llama-3.2-3B-Instruct:source=Llama3"
    add_bos: true
  - pretrained_model_name_or_path: "benjamin/Qwen2.5-1.5B-Instruct-flax"
    tokenizer_name: "Qwen/Qwen2.5-1.5B:source=Qwen2"
    add_bos: false
  - pretrained_model_name_or_path: "benjamin/gemma-2-2b-it-flax"
    tokenizer_name: "google/gemma-2-2b-it:source=Gemma2"
    add_bos: true

baseline_mined_mapping_paths: [null, "artifacts/tokenizer_data/qwen2_to_llama3", "artifacts/tokenizer_data/gemma2_to_llama3"]